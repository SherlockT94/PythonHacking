import requests
import re
import urllib
from bs4 import BeautifulSoup

class Scanner:
    def __init__(self, url, ignore_links):
        self.session = requests.Session()# return a Session object
        self.target_url = url
        self.target_links = []
        self.links_to_ignore = ignore_links
        
    def extract_link_from(self, url):
        response = self.session.get(url)# Using session object's get() to make sure we are at the same session and would not be logout
        # if utf-8 error: using str() instead of decode() to transfer b" to a string start with b"
        return re.findall('(?:href=")(.*?)"', str(response.content))

    # When discover a link, it will go into the link utill there is no more to go
    # Using default value: if I use target.url to replace url, when it call itself, it will start with 'href_links = self.extract_link_from(self.target_url)' and it never go deeper to discover the website
    def crawl(self, url=None):
        if url == None:
            url = self.target_url
        href_links = self.extract_link_from(url)
        for link in href_links:
            link = urllib.parse.urljoin(url, link)#join relative url and target url
            if "#" in link:
                link = link.split("#")[0]
            if self.target_url in link and link not in self.target_links and link not in self.links_to_ignore:
                self.target_links.append(link)# append() will searching the global namespace, so no need to using global keyword
                print(link)
                self.crawl(link)# call it self

    def extract_forms(self, url):
        response = self.session.get(url)
        parsed_html = BeautifulSoup(response.content, features="lxml")# set lxml as the HTML parser, if not set, there will be a warning
        return parsed_html.find_all("form")# find all form tag in current page
    
    def submit_form(self, form, value, url):
        action = form.get("action")#get html attribute
        post_url = urllib.parse.urljoin(url, action)
        method = form.get("method")

        inputs_list = form.find_all("input")
        post_data = {}
        for input_tag in inputs_list:
            input_name = input_tag.get("name")
            input_type = input_tag.get("type")
            input_value = input_tag.get("value")
            if input_type == "text":
                input_value = value

            post_data[input_name] = input_value
        # Some forms using GET as method
        if method == "post":
            return self.session.post(post_url, data=post_data)
        return self.session.get(post_url, params=post_data)# params is the values of inputs when using GET method
    
    def run_scanner(self):
        for link in self.target_links:
            forms = self.extract_forms(link)
            for form in forms:
                print("[+] Testing form in " + link)
                is_vulnberable_to_xss = self.test_xss_in_form(form, link)
                if is_vulnberable_to_xss:
                    print("\n\n[***] XSS discovered in " + link + " in the following form")
                    print(form)

            if "=" in link:
                print("\n\n[+] Testing " + link)
                is_vulnberable_to_xss = self.test_xss_in_link(link)
                if is_vulnberable_to_xss:
                    print("[***] XSS discovered in " + link)

    def test_xss_in_link(self, url):
        xss_test_script = "<sCript>alert('test')</scriPt>"
        url = url.replace("=", "=" + xss_test_script)
        response = self.session.get(url)
        # if xss_include.php_script in response.content.decode():
            # return True
        return xss_test_script in response.content.decode()# better way 

    def test_xss_in_form(self, form, url):
        xss_test_script = "<sCript>alert('test')</scriPt>"
        response = self.submit_form(form, xss_test_script, url)
        # if xss_test_script in response.content.decode():
            # return True
        return xss_test_script in response.content.decode()
